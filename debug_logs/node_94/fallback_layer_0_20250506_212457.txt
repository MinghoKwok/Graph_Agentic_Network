ðŸ“¤ [DEBUG] Fallback Prompt for Node 94 | Layer 0 | 20250506_212457:
You are a label prediction agent.

You will be given a set of labeled memory items and a new node to classify.
Each example includes a few labeled texts as memory and a new text to classify.
Use the memory to predict the label for the current text.

Think step-by-step. Consider which label is best supported by both the semantic content of the node text and the examples in memory.
Do not rely on abstract or popular terms alone (like "system" or "accuracy") unless those match the label examples provided.

## Example 1:
Memory:
1. [Label_2] "Hidden Markov models for sequence modeling and pattern discovery."
2. [Label_1] "Neural networks for text classification."
3. [Label_2] "Bayesian models for probabilistic inference."
Current Node Text:
"Markov models for speech sequence alignment."
Prediction: Label_2
Reasoning: The text discusses Markov models and speech alignment, which closely match Label_2 examples in memory.

## Example 2:
Memory:
1. [Label_2] "Hidden Markov models for biological sequence alignment."
2. [Label_6] "Improving ensemble model selection with probabilistic voting."
3. [Label_2] "Bayesian inference for protein sequence homology detection."
4. [Label_6] "Boosted decision trees for structured data classification."
5. [Label_3] "Non-reversible Markov chains for MCMC sampling."
Current Node Text:
"Homology detection in genetic sequences using Bayesian Markov modeling."
Prediction: Label_2
Reasoning: Although both Label_2 and Label_6 are well represented in memory, the current node text focuses on homology detection and Bayesian modeling, which strongly aligns with Label_2 examples related to biological sequences and probabilistic inference, rather than ensemble or structured classifiers.

## Your Turn:
Memory Summary:
- Label_3: 10 examples
- Label_4: 1 examples

Memory:
1. [Label_3] "Analysis of a Non-Reversible Markov Chain Sampler Abstract: Technical Report BU-1385-M, Biometrics Unit, Cornell University Abstract We analyse the co..."
2. [Label_3] "On the Markov Equivalence of Chain Graphs, Undirected Graphs, and Acyclic Digraphs Abstract: Graphical Markov models use undirected graphs (UDGs), acy..."
3. [Label_3] "Analysis of the Gibbs sampler for a model related to James-Stein estimators Abstract: Summary. We analyze a hierarchical Bayes model which is related ..."
4. [Label_3] "Hidden Markov decision trees Abstract: We study a time series model that can be viewed as a decision tree with Markov temporal structure. The model is..."
5. [Label_4] "Reinforcement Learning Algorithm for Partially Observable Markov Decision Problems Abstract: Increasing attention has been paid to reinforcement learn..."

Text to classify:
"Perfect Simulation in Stochastic Geometry
Abstract: Simulation plays an important role in stochastic geometry and related fields, because all but the simplest random set models tend to be intractable to analysis. Many simulation algorithms deliver (approximate) samples of such random set models, for example by simulating the equilibrium distribution of a Markov chain such as a spatial birth-and-death process. The samples usually fail to be exact because the algorithm simulates the Markov chain for a long but finite time, and thus convergence to equilibrium is only approximate. The seminal work by Propp and Wilson made an important contribution to simulation by proposing a coupling method, Coupling from the Past (CFTP), which delivers perfect, that is to say exact, simulations of Markov chains. In this paper we introduce this new idea of perfect simulation and illustrate it using two common models in stochastic geometry: the dead leaves model and a Boolean model conditioned to cover a finite set of points."

Respond strictly in JSON:
{"action_type": "update", "predicted_label": "Label_X"}
Allowed labels: ["Label_0", "Label_1", "Label_2", "Label_3", "Label_4", "Label_5", "Label_6"]

================================================================================