
üì§ [DEBUG] Action Prompt for Node 209 | Layer 0 | 20250506_212520:

    You are Node 209 in a scientific citation network. Your task is to classify yourself into the correct research category based on your text and connections.
    
    ## Few-shot Examples of Label Prediction:

    Example 1:
    Memory:
    1. [Neural_Networks] "A novel deep learning approach for image classification..."
    2. [Reinforcement_Learning] "Q-learning based algorithm for game playing..."
    3. [Neural_Networks] "Convolutional neural networks for computer vision tasks..."

    Current Node Text:
    "Deep learning models for visual recognition tasks..."

    Prediction: Neural_Networks
    Reasoning: The current text focuses on deep learning and visual recognition, which closely matches the Neural_Networks examples in memory.

    Example 2:
    Memory:
    1. [Probabilistic_Methods] "Bayesian networks for uncertainty modeling..."
    2. [Neural_Networks] "Recurrent neural networks for sequence prediction..."
    3. [Probabilistic_Methods] "Markov models for time series analysis..."

    Current Node Text:
    "Hidden Markov models for speech recognition..."

    Prediction: Probabilistic_Methods
    Reasoning: The text discusses Markov models, which is a probabilistic method, matching the Probabilistic_Methods examples in memory.

    Example 3:
    Memory:
    1. [Neural_Networks] "Deep learning architectures for natural language processing..."
    2. [Theory] "Theoretical analysis of algorithm complexity..."
    3. [Neural_Networks] "Transformer models for sequence modeling..."

    Current Node Text:
    "Attention mechanisms in deep learning models for text understanding..."

    Prediction: Neural_Networks
    Reasoning: Although the text mentions theoretical concepts like attention mechanisms, the focus is on deep learning models and their application to text understanding, which closely matches the Neural_Networks examples in memory.
    
    ## Your State:
    - Node ID: 209
    - Layer: 0
    - Your Text:
    "17 Massively Parallel Genetic Programming
Abstract: As the field of Genetic Programming (GP) matures and its breadth of application increases, the need for parallel implementations becomes absolutely necessary. The transputer-based system presented in the chapter by Koza and Andre ([11]) is one of the rare such parallel implementations. Until today, no implementation has been proposed for parallel GP using a SIMD architecture, except for a data-parallel approach ([20]), although others have exploited workstation farms and pipelined supercomputers. One reason is certainly the apparent difficulty of dealing with the parallel evaluation of different S-expressions when only a single instruction can be executed at the same time on every processor. The aim of this chapter is to present such an implementation of parallel GP on a SIMD system, where each processor can efficiently evaluate a different S-expression. We have implemented this approach on a MasPar MP-2 computer, and will present some timing results. To the extent that SIMD machines, like the MasPar are available to offer cost-effective cycles for scientific experimentation, this is a useful approach. The idea of simulating a MIMD machine using a SIMD architecture is not new ([8, 15]). One of the original ideas for the Connection Machine ([8]) was that it could simulate other parallel architectures. Indeed, in the extreme, each processor on a SIMD architecture can simulate a universal Turing machine (TM). With different turing machine specifications stored in each local memory, each processor would simply have its own tape, tape head, state table and state pointer, and the simulation would be performed by repeating the basic TM operations simultaneously. Of course, such a simulation would be very inefficient, and difficult to program, but would have the advantage of being really MIMD, where no SIMD processor would be in idle state, until its simulated machine halts. Now let us consider an alternative idea, that each SIMD processor would simulate an individual stored program computer using a simple instruction set. For each step of the simulation, the SIMD system would sequentially execute each possible instruction on the subset of processors whose next instruction match it. For a typical assembly language, even with a reduced instruction set, most processors would be idle most of the time. However, if the set of instructions implemented on the virtual processor is very small, this approach can be fruitful. In the case of Genetic Programming, the "instruction set" is composed of the specified set of functions designed for the task. We will show below that with a precompilation step, simply adding a push, a conditional, and unconditional branching and a stop instruction, we can get a very effective MIMD simulation running. This chapter reports such an implementation of GP on a MasPar MP-2 parallel computer. The configuration of our system is composed of 4K processor elements"
    - Neighbors: [415, 2334, 2704]
    - Available labeled neighbors to retrieve from: None
    - Neighbors with predicted labels: None
    ‚ö†Ô∏è You already have a label: "1". You may consider broadcasting this label and your text to your neighbors to help them in their predictions.
    You are an autonomous agent with planning capabilities. You may perform multiple actions in sequence to achieve better results.

    ## Decide Your Next Action(s)
    Important: You are allowed and encouraged to return MULTIPLE actions in sequence. You MUST respond with a JSON array even if there's only one action. 
    Example of a valid response:
    ```json
    [
      {"action_type": "update", "predicted_label": "Neural_Networks"},
      {"action_type": "broadcast"}
    ]
    ```
    ```json
    [
      {"action_type": "retrieve", "target_nodes": [1, 2, 3], "info_type": "text"},
      {"action_type": "rag_query", "query": "machine learning", "top_k": 10}
    ]
    ```
    Invalid response:
    ```json
    {"action_type": "update", "predicted_label": "Neural_Networks"}
    ```

    ### Available Actions:

    1. "retrieve": get information from other nodes
    - Format: {"action_type": "retrieve", "target_nodes": [IDs], "info_type": "text"}

    2. "broadcast": send a message to neighbors if and *only* if you already have a label or predicted label
    - Format: {"action_type": "broadcast", "target_nodes": [IDs], "message": "some message"}
    - Use this *only* when you already have a label orpredicted label to share it with neighbors. 
    - You MUST NOT use "broadcast" unless you already have a label orpredicted label (i.e., after an "update" action).
    - So "update" action always works before "broadcast" in the same layer.

    
    4. "rag_query": search globally for similar labeled nodes, can make up "retrieve" action
    - Format: {"action_type": "rag_query", "query": [Your node ID, e.g. 13/57], "top_k": number of nodes to retrieve}
    - Use this when you don't have enough informative neighbors or memory, and need global examples.
    - You must use your own node ID as the query.

    5. "no_op": take no action
    - Format: {"action_type": "no_op"}

    

    ## Planning Your Steps
    1. If you have a predicted label, you can choose to broadcast it or continue to retrieve nodes with labels.
    2. If you don't have a predicted label, think like a planner: first gather evidence (retrieve, rag_query), then make a decision (update), and finally help others (broadcast).
    Think about the following:
    - If you cannot predict your label yet, need more context to predict your label ‚Üí `retrieve`, `rag_query`
    - Are you confident to predict your label? ‚Üí `update`
    - Have you shared your label or predicted label with neighbors? ‚Üí `broadcast`
    - Only broadcast if you have a predicted label or training label, AND your memory is not empty. If not, choose "retrieve" or "rag_query" first.
    - If any neighbors already have predicted labels, it is recommended to retrieve from them first.
    
================================================================================