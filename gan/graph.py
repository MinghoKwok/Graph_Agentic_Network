"""
Graph structure and network implementation for Graph Agentic Network
"""

import torch
import numpy as np
from typing import Dict, List, Any, Optional, Tuple, Union
from tqdm import tqdm
import json
import os

import config
from gan.node import NodeState, NodeAgent


import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


class AgenticGraph:
    """Graph structure for agentic network."""

    def __init__(self, adj_matrix, llm_interface, labels=None, train_idx=None, dataset_name=None):
        """
        Initialize the graph with adjacency matrix and node agents.
        
        Args:
            adj_matrix: Adjacency matrix of the graph
            llm_interface: LLM interface for node agents
            labels: Optional tensor of node labels
            train_idx: Optional tensor of training node indices
            dataset_name: Name of the dataset (defaults to config.DATASET_NAME)
        """
        self.adj_matrix = adj_matrix
        self.num_nodes = adj_matrix.shape[0]
        self.dataset_name = dataset_name or config.DATASET_NAME
        
        # Load node texts from JSONL file
        dataset_config = config.DATASET_CONFIGS[self.dataset_name]
        jsonl_path = os.path.join("..", "data", self.dataset_name, dataset_config["text_graph_file"])
        with open(jsonl_path) as f:
            records = [json.loads(l) for l in f]
        node_texts = {r["node_id"]: r["text"] for r in records}
        self._node_texts = node_texts
        
        # Create train mask if train_idx is provided
        train_mask = torch.zeros(self.num_nodes, dtype=torch.bool)
        if train_idx is not None:
            train_mask[train_idx] = True
            
        # Initialize node agents
        self.nodes = {}
        for i in range(self.num_nodes):
            # Only keep labels for training nodes
            if labels is not None and train_mask[i]:
                node_label = torch.tensor(int(labels[i]))
            else:
                node_label = None
            text = node_texts.get(i, "")
            state = NodeState(
                node_id=i,
                text=text,
                label=node_label
            )
            self.nodes[i] = NodeAgent(state, llm_interface)

        # Initialize vector index
        self._initialize_vector_index()

    def _initialize_vector_index(self):
        from sklearn.neighbors import NearestNeighbors
        import numpy as np
        import os
        # import pdb; pdb.set_trace()

        dataset_config = config.DATASET_CONFIGS[self.dataset_name]
        emb_path = os.path.join("..", "data", self.dataset_name, dataset_config["embeddings_file"])
        if not os.path.exists(emb_path):
            print(f"âŒ Missing embedding file: {emb_path}")
            self._node_features_index = None
            return

        # Load embeddings
        print(f"[DEBUG] æ­£åœ¨åŠ è½½ embedding æ–‡ä»¶: {emb_path}")
        node_embeddings = np.load(emb_path, allow_pickle=True).item()
        
        # [DEBUG] æ£€æŸ¥ embedding æ•°æ®ç»“æ„
        if not isinstance(node_embeddings, dict):
            print(f"[DEBUG] âš ï¸ è­¦å‘Š: embeddings ä¸æ˜¯ dict ç±»å‹ï¼Œè€Œæ˜¯ {type(node_embeddings)}")
            # å°è¯•è½¬æ¢
            if hasattr(node_embeddings, 'item'):
                try:
                    node_embeddings = node_embeddings.item()
                    if not isinstance(node_embeddings, dict):
                        print(f"[DEBUG] âš ï¸ è½¬æ¢åä»ä¸æ˜¯ dict: {type(node_embeddings)}")
                except:
                    print(f"[DEBUG] âŒ è½¬æ¢ embeddings åˆ° dict å¤±è´¥")
        
        # [DEBUG] æ£€æŸ¥ embedding å†…å®¹
        print(f"[DEBUG] Embedding é”®ç±»å‹: {type(list(node_embeddings.keys())[0]) if node_embeddings and len(node_embeddings) > 0 else 'N/A'}")
        print(f"[DEBUG] Embedding å€¼ç±»å‹: {type(list(node_embeddings.values())[0]) if node_embeddings and len(node_embeddings) > 0 else 'N/A'}")
        print(f"[DEBUG] Embedding å€¼å½¢çŠ¶: {list(node_embeddings.values())[0].shape if node_embeddings and len(node_embeddings) > 0 and hasattr(list(node_embeddings.values())[0], 'shape') else 'N/A'}")
        
        # [DEBUG] æ£€æŸ¥ embedding é”®å€¼çš„èŒƒå›´
        if node_embeddings and len(node_embeddings) > 0:
            min_key = min(node_embeddings.keys())
            max_key = max(node_embeddings.keys())
            print(f"[DEBUG] Embedding é”®èŒƒå›´: [{min_key}, {max_key}]")
            
            # æ£€æŸ¥æ•°å€¼èŒƒå›´æ˜¯å¦åˆç†
            sample_vec = list(node_embeddings.values())[0]
            if hasattr(sample_vec, 'min') and hasattr(sample_vec, 'max'):
                print(f"[DEBUG] Embedding å€¼èŒƒå›´: [{sample_vec.min():.4f}, {sample_vec.max():.4f}]")
                print(f"[DEBUG] Embedding å¹³å‡å€¼: {sample_vec.mean():.4f}")
        
        self._node_id_to_query_vector = node_embeddings
        print(f"âœ… Loaded sentence-transformer embeddings for {len(node_embeddings)} nodes")

        # Build index (only for training nodes)
        node_features = []
        node_indices = []
        
        # [DEBUG] æ£€æŸ¥è®­ç»ƒèŠ‚ç‚¹æ•°é‡
        labeled_nodes_count = sum(1 for agent in self.nodes.values() if agent.state.label is not None)
        print(f"[DEBUG] æœ‰æ ‡ç­¾çš„è®­ç»ƒèŠ‚ç‚¹æ•°é‡: {labeled_nodes_count}")
        
        missing_embedding_count = 0
        node_id_mismatch_count = 0
        for node_id, agent in self.nodes.items():
            if agent.state.label is not None:
                # æ£€æŸ¥èŠ‚ç‚¹IDæ˜¯å¦åœ¨embeddingé”®ä¸­å­˜åœ¨
                if node_id not in node_embeddings:
                    node_id_mismatch_count += 1
                    if node_id_mismatch_count <= 5:
                        print(f"[DEBUG] âš ï¸ èŠ‚ç‚¹IDä¸åŒ¹é…: {node_id} ä¸åœ¨embeddingé”®ä¸­")
                    
                vec = node_embeddings.get(node_id)
                if vec is not None:
                    node_features.append(vec)
                    node_indices.append(node_id)
                else:
                    missing_embedding_count += 1
                    if missing_embedding_count <= 5:
                        print(f"[DEBUG] âš ï¸ èŠ‚ç‚¹ {node_id} ç¼ºå°‘embeddingå‘é‡")
        
        if missing_embedding_count > 0:
            print(f"[DEBUG] âš ï¸ {missing_embedding_count} ä¸ªæœ‰æ ‡ç­¾èŠ‚ç‚¹ç¼ºå°‘ embedding")
        if node_id_mismatch_count > 0:
            print(f"[DEBUG] âš ï¸ {node_id_mismatch_count} ä¸ªæœ‰æ ‡ç­¾èŠ‚ç‚¹çš„IDä¸åœ¨embeddingé”®ä¸­")
        
        self._feature_idx_to_node_id = {i: nid for i, nid in enumerate(node_indices)}

        if len(node_features) > 0:
            print(f"[DEBUG] æ„å»ºNearestNeighborsç´¢å¼•ï¼Œä½¿ç”¨ {len(node_features)} ä¸ªå‘é‡")
            
            # æ£€æŸ¥ç‰¹å¾å‘é‡æ˜¯å¦æœ‰NaNæˆ–å¼‚å¸¸å€¼
            stacked_features = np.stack(node_features)
            if np.isnan(stacked_features).any():
                print(f"[DEBUG] âš ï¸ ç‰¹å¾å‘é‡ä¸­åŒ…å«NaNå€¼!")
            if np.isinf(stacked_features).any():
                print(f"[DEBUG] âš ï¸ ç‰¹å¾å‘é‡ä¸­åŒ…å«Infå€¼!")
            
            # æ£€æŸ¥å‘é‡å½’ä¸€åŒ–
            norms = np.linalg.norm(stacked_features, axis=1)
            print(f"[DEBUG] å‘é‡èŒƒæ•°ç»Ÿè®¡: min={norms.min():.4f}, max={norms.max():.4f}, mean={norms.mean():.4f}")
            
            # å¦‚æœå‘é‡èŒƒæ•°å·®å¼‚è¿‡å¤§ï¼Œå¯ä»¥è€ƒè™‘å½’ä¸€åŒ–
            if norms.max() / norms.min() > 10:
                print(f"[DEBUG] âš ï¸ å‘é‡èŒƒæ•°å·®å¼‚è¾ƒå¤§ï¼Œå¯èƒ½å½±å“ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—")
                print(f"[DEBUG] è€ƒè™‘å¯¹å‘é‡è¿›è¡Œå½’ä¸€åŒ–å¤„ç†")
            
            self._node_features_index = NearestNeighbors(
                n_neighbors=min(10, len(node_features)),
                metric='cosine'
            )
            self._node_features_index.fit(stacked_features)
            print(f"âœ… RAG index built with {len(node_features)} labeled candidates")
        else:
            print("âš ï¸ No valid labeled nodes for RAG index")
            self._node_features_index = None


    def rag_query(self, query_id: int, top_k: int = 5) -> Dict[int, Dict[str, Any]]:
        # import pdb; pdb.set_trace()
        try:
            node_id = query_id
            query_vector = self._node_id_to_query_vector.get(node_id)
            if query_vector is None:
                print(f"âš ï¸ Node {node_id} has no query vector.")
                return {}
            
            # [DEBUG] æ‰“å°æŸ¥è¯¢å‘é‡ä¿¡æ¯
            print(f"[DEBUG] Query vector for node {node_id}:")
            print(f"[DEBUG] Vector shape: {query_vector.shape}, Type: {type(query_vector)}")
            print(f"[DEBUG] Vector norm: {np.linalg.norm(query_vector):.4f}")
            
            query_vector = query_vector.reshape(1, -1)
        except (ValueError, TypeError) as e:
            print(f"âš ï¸ RAG query error: {e}")
            print(f"âš ï¸ RAG query must be a node ID. Got: {query_id}")
            return {}

        if self._node_features_index is None:
            print("âš ï¸ Vector index not available for RAG query")
            return {}

        distances, indices = self._node_features_index.kneighbors(
            query_vector, n_neighbors=min(top_k, self._node_features_index.n_samples_fit_)
        )
        
        # [DEBUG] æ‰“å°ç´¢å¼•å†…å®¹ä¿¡æ¯
        print(f"[DEBUG] Nearest neighbors index info:")
        print(f"[DEBUG] Index size: {self._node_features_index.n_samples_fit_}")
        print(f"[DEBUG] Feature map size: {len(self._feature_idx_to_node_id)}")

        results = {}
        print(f"[DEBUG] è¯¦ç»†ç›¸ä¼¼åº¦æ’å:")
        for i, idx in enumerate(indices[0]):
            result_node_id = self._feature_idx_to_node_id.get(idx)
            if result_node_id is not None:
                node = self.get_node(result_node_id)
                similarity_score = -float(distances[0][i])
                print(f"[DEBUG]   Rank {i+1}: Node {result_node_id} | Score: {similarity_score:.4f}")
                if node and node.state.label is not None:
                    result_vector = self._node_id_to_query_vector.get(result_node_id)
                    if result_vector is not None:
                        # é¢å¤–æ‰“å°å‘é‡ä¿¡æ¯å’Œç›¸ä¼¼åº¦è®¡ç®—ç»†èŠ‚
                        cos_sim = np.dot(query_vector.flatten(), result_vector.flatten()) / (np.linalg.norm(query_vector) * np.linalg.norm(result_vector))
                        print(f"[DEBUG]   - Label: {node.state.label.item()} | Vector norm: {np.linalg.norm(result_vector):.4f} | Cosine sim: {cos_sim:.4f}")
                    
                    results[result_node_id] = {
                        'text': self._node_texts.get(result_node_id, ""),
                        'label': node.state.label.item(),
                        'similarity_score': similarity_score
                    }
        
        print(f"ğŸ” RAG Query: Node {node_id}")
        print(f"ğŸ“Š Candidate pool size: {len(self._feature_idx_to_node_id)}")
        print(f"ğŸ“ Top-{top_k} results:")
        for result_id, result in results.items():
            print(f"  â†³ Node {result_id} | Label: {result['label']} | Score: {result['similarity_score']:.3f}")

        return results

    def has_node(self, node_id: int) -> bool:
        """
        Check whether the node exists in the graph.
        
        Args:
            node_id: Node ID to check
        
        Returns:
            True if node exists, False otherwise
        """
        return node_id in self.nodes

    def get_node(self, node_id: int) -> NodeAgent:
        """
        Get a node agent by ID.
        
        Args:
            node_id: ID of the node
            
        Returns:
            The node agent object
        """
        return self.nodes.get(node_id)
    
    def get_neighbors(self, node_id: int) -> List[int]:
        """
        Get neighbors of a node.
        
        Args:
            node_id: ID of the node
            
        Returns:
            List of neighbor node IDs
        """
        neighbors = torch.where(self.adj_matrix[node_id] > 0)[0].tolist()
        
        # Limit number of neighbors if too many
        if len(neighbors) > config.MAX_NEIGHBORS:
            neighbors = neighbors[:config.MAX_NEIGHBORS]
            
        return neighbors
    
    def run_layer(self, layer: int, node_indices: Optional[List[int]] = None) -> None:
        """
        Run one layer of agent processing on the graph.
        
        Args:
            layer: Layer number
            node_indices: Optional subset of nodes to process (for batching)
        """
        # Process only specified nodes or all nodes
        if node_indices is None:
            node_indices = list(self.nodes.keys())
        
        for node_id in tqdm(node_indices, desc=f"Processing layer {layer}"):
            agent = self.nodes[node_id]
            agent.step(self, layer)
        
        # After all nodes have executed actions, prepare for next layer
        for node_id in node_indices:
            agent = self.nodes[node_id]
            agent.state.increment_layer()
            agent.state.clear_messages()  # Clear messages to avoid memory buildup

    def initialize_labels_and_index(self, train_idx: List[int], labels: torch.Tensor):
        """
        åˆå§‹åŒ–è®­ç»ƒé›†æ ‡ç­¾å¹¶æ„å»ºå‘é‡ç´¢å¼•ã€‚
        
        Args:
            train_idx: è®­ç»ƒé›†èŠ‚ç‚¹IDåˆ—è¡¨
            labels: èŠ‚ç‚¹æ ‡ç­¾å¼ é‡
        """
        # å†™å…¥è®­ç»ƒé›†æ ‡ç­¾
        for nid in train_idx:
            if nid in self.nodes:
                self.nodes[nid].state.label = int(labels[nid])
        
        # åˆå§‹åŒ–å‘é‡ç´¢å¼•
        self._initialize_vector_index()


class GraphAgenticNetwork:
    """Main class for the Graph Agentic Network framework."""
    
    def __init__(self, 
                adj_matrix: torch.Tensor, 
                node_texts: List[str],
                llm_interface: 'LLMInterface',
                labels: Optional[torch.Tensor] = None,
                num_layers: int = config.NUM_LAYERS,
                train_idx: Optional[List[int]] = None,
                dataset_name: str = config.DATASET_NAME):  # æ·»åŠ  dataset_name å‚æ•°
        """
        Initialize the Graph Agentic Network.
        
        Args:
            adj_matrix: Adjacency matrix of the graph
            node_texts: List of node text descriptions
            llm_interface: LLM interface for node agents
            labels: Optional tensor of node labels
            num_layers: Number of GAN layers to run
            train_idx: Optional list of training node indices
            dataset_name: Name of the dataset (defaults to config.DATASET_NAME)
        """
        self.graph = AgenticGraph(
            adj_matrix=adj_matrix,
            llm_interface=llm_interface,
            labels=labels,
            train_idx=train_idx,
            dataset_name=dataset_name
        )
        self.num_layers = num_layers
        self.train_idx = train_idx
        self.llm_interface = llm_interface
        self.current_layer = 0
    
    def forward(self, batch_size: Optional[int] = None) -> None:
        """
        Run the Graph Agentic Network for the specified number of layers.
        
        Args:
            batch_size: Optional batch size for processing nodes
        """
        all_nodes = list(range(self.graph.num_nodes))
        
        for layer in range(self.num_layers):
            self.current_layer = layer
            print(f"Processing layer {layer+1}/{self.num_layers}")
            
            # Process in batches if specified
            if batch_size is not None and batch_size < self.graph.num_nodes:
                # Divide nodes into batches
                num_batches = (self.graph.num_nodes + batch_size - 1) // batch_size
                for i in range(num_batches):
                    start_idx = i * batch_size
                    end_idx = min((i + 1) * batch_size, self.graph.num_nodes)
                    batch_nodes = all_nodes[start_idx:end_idx]
                    
                    self.graph.run_layer(layer, batch_nodes)
            else:
                # Process all nodes at once
                self.graph.run_layer(layer)
    
    def get_node_predictions(self) -> torch.Tensor:
        """
        Get the node predictions (e.g., for node classification).
        
        Returns:
            Tensor of predicted labels
        """
        predictions = torch.zeros(self.graph.num_nodes, dtype=torch.long)
        
        for node_id, agent in self.graph.nodes.items():
            if agent.state.predicted_label is not None:
                predictions[node_id] = agent.state.predicted_label
        
        return predictions
    
    def get_node_representations(self) -> Dict[int, torch.Tensor]:
        """
        Get the final node representations.
        
        Returns:
            Dictionary mapping node IDs to their hidden state
        """
        return {node_id: agent.state.hidden_state.clone() 
                for node_id, agent in self.graph.nodes.items()}
    
    def get_node_memory(self) -> Dict[int, List[Dict]]:
        """
        Get the memory of each node.
        
        Returns:
            Dictionary mapping node IDs to their memory list
        """
        return {node_id: agent.state.memory.copy() 
                for node_id, agent in self.graph.nodes.items()}
