You are given a scientific paper text and several labeled examples. Predict the most likely labe
l of your text.                                                                                 
                                                                                                
Text to classify:               
Locally Linear Experts Cooperate for Incremental Regression Learning                                                                   

Aggregated_Text from neighbors:
""Incremental learning system with locally linear experts""


Memory:
from neighbors:
"text": "\"Applying EM Algorithm to Hierarchical Mixture Models for Supervised Learning.\"", "label": "Probabilistic_Methods"
"text": "Gain-adaptation algorithms outperform traditional methods in stochastic time-varying linear systems.", "label": "Neural_Networks"
from rag:
'text': 'Learning Unions of Rectangles with Queries.', 'label': 6, 'label_text': 'theory', 
 'text': 'Constructive induction from data in AQ17-DCI is further explored through experiments.', 'label': 5, 'label_text': 'Rule_Learning', 'source': 1292, 
 'text': 'Machine learning approaches beyond traditional methods.', 'label': 2, 'label_text': 'Neural_Networks', 'source': 908, 
 'text': 'Compositional modeling using Deep Neural Networks (DPNs).', 'label': 3, 'label_text': 'Probabilistic_Methods', 'source': 905, 
 'text': 'Data-Driven Reasoning', 'label': 0, 'label_text': 'Case_Based', 

                                                                                                
                                                                                                
You are given the scientific paper text and the official definitions of candidate labels.       
Your task is to predict the most appropriate label for the given text.    
Your feature is your text to classify and aggregated text. The memory is important for you to reference. 
In memory, items having similar text with you are more likely to have the same label/class with you.
Generally, your label is more likely to occur in memory items. And the more occurence one label has, the probability that you have the same label.
If any memory items share strong semantic similarity and have consistent labels, you should prioritize their label even if the text is ambiguous.
IMPORTANT: The priority for reference is: **memory from neighbor > memory from rag > definition** (consider momory firstly, then consider definition)
Consider definition if and only if you don't have memory items.                                           
                                                                                                
Here are the official label definitions:                                                        
                                                                                                
[label=Theory]                                                                                  
- Definition:                                                                                   
    - Focuses on the development of abstract learning models and formal theoretical frameworks. 
    - Analyzes generalization bounds, computational complexity, learnability, or approximation l
imits.                                                                                          
    - Does not involve specific network architectures, training processes, or real-world applica
tion details.                                                                                           
- Examples:                                                                                     
    - "Analyzing the PAC learnability of Boolean concept classes under limited samples."        
    - "Exploring convergence bounds for support vector machines in high-dimensional spaces."    
                                                                                                
[label=Neural_Networks]                                                                         
- Definition:                                                                                   
    - Focuses on the design, training, and application of multi-layered network models (e.g., CN
Ns, RNNs, feedforward networks).                                                                
    - Includes training optimization, architecture development, and empirical performance evalua
tion.                                                                                           
    - If the text describes any specific neural network structure, learning method, or applied u
sage, classify here.                                                                            
- Examples:                                     
    - "Improving object detection using deep convolutional neural networks."
    - "Training recurrent neural networks for language modeling tasks."

[label=Case_Based]                              
- Definition:                                   
    - Solves new problems by adapting solutions from previously solved cases.
    - Highlights memory-based reasoning, example retrieval, and adaptation processes.
- Examples:                                     
    - "Using past legal cases to inform decisions on new disputes."
    - "Adapting historical mechanical fault diagnoses for new industrial equipment."

[label=Genetic_Algorithms]                                                                      
- Definition:                                   
    - Optimization methods inspired by biological evolution.
    - Involves selection, mutation, crossover, and evolutionary adaptation.
- Examples:                                     
    - "Optimizing urban traffic flow using genetic algorithm techniques."
    - "Designing efficient neural network architectures with evolutionary strategies."

[label=Probabilistic_Methods]                                                                   
- Definition:                                   
    - Models uncertainty using probability theory and Bayesian frameworks.
    - Includes graphical models, probabilistic inference, and decision making under uncertainty.
- Examples:                                     
    - "Applying Bayesian networks for disease risk prediction from incomplete clinical data."
    - "Using probabilistic graphical models to infer social network influence patterns."

[label=Reinforcement_Learning]                                                                  
- Definition:                                   
    - Learns optimal behaviors through trial and error with environmental rewards.
    - Formalized as Markov Decision Processes (MDPs), balancing exploration and exploitation.
- Examples:                                     
    - "Training an agent to navigate a maze environment using Q-learning."
    - "Optimizing robotic grasping strategies via reinforcement learning."

[label=Rule_Learning]                           
- Definition:                                   
    - Extracts symbolic "if-then" rules from training data for classification and reasoning.
    - Produces interpretable and compact decision logic.
- Examples:                                     
    - "Learning decision rules for customer churn prediction."
    - "Extracting symbolic classification rules for fraud detection."


Important Decision Rules:                       
- If the text mentions neural network structures, training behaviors, or applied tasks, even alo
ngside theoretical discussion, classify as Neural_Networks.
- If the text only discusses abstract theoretical limits, generalization theory, or computationa
l complexity without any specific model, classify as Theory.
- If unsure, prioritize the definition that explicitly matches the described methods, models, or
 systems.
- Do not classify based solely on words like "optimization" or "convergence" without considering
 the context.

[Example]
Text to classify: Changing Supply Functions in Input/State Stable
Memory:
from your neighbors:
["text": "Input to State Stability property characterizations.", "label": label_2]
from rag:
'text': 'Characterizations of learnability for classes of functions with finite non-negative values.', 'label': label_6
'text': 'Automated fitness raters are developed for the GP-Music system to evaluate and improve musical compositions generated by genetic programming.', 'label': label_1,
'text': 'Language as a complex, self-organizing system.', 'label': label_2,
'text': 'Integration of functions with dominant peaks in subregions.', 'label': label_3,
'text': 'A Genetic Algorithm for Economic Modeling.', 'label': label_1
Prediction:
We have momory, so we almost don't need to consider definition of label.
"State Stable" topic in original text is so similar with the text in memory "Input to State Stability property characterizations.". So we predict that the label is the same with it(label_2).




Respond in JSON format:                         
{"action_type": "update", "predicted_label": "label_string"}

                                                
Respond with:                                   
{"action_type": "update", "predicted_label": "label_string"}
You must choose one of the allowed label strings exactly as listed: ["Case_Based", "Genetic_Algo
rithms", "Neural_Networks", "Probabilistic_Methods", "Reinforcement_Learning", "Rule_Learning", 
"Theory"] for your text, based on your text to classify, aggregated text, memory and label definitions.                                                                      

Your original text is more important to define your feature than aggregated text.
The predicted label should have occurred in memory, unless the memory is empty.  
